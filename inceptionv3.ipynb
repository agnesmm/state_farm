{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 10 classes.\n",
      "Found 20 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dense, Input, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "file = 'mini_sample'\n",
    "if file=='all':\n",
    "    path = '/home/ubuntu/nbs/state_farm/data/'\n",
    "    batch_size=64\n",
    "    nb_train_samples = 34000\n",
    "    nb_validation_samples = 4800\n",
    "elif file=='sample':\n",
    "    path = '/home/ubuntu/nbs/state_farm/data/sample/'\n",
    "    batch_size=64\n",
    "    nb_train_samples = 1600\n",
    "    nb_validation_samples = 300\n",
    "elif file=='mini_sample':\n",
    "    path = '/home/ubuntu/nbs/state_farm/data/mini_sample/'\n",
    "    batch_size=10\n",
    "    nb_train_samples = 200\n",
    "    nb_validation_samples = 20\n",
    "else:\n",
    "    print('Unknown file name')\n",
    "    exit()\n",
    "\n",
    "#path = '/home/ubuntu/nbs/state_farm/data/'\n",
    "target_size=(224, 224)\n",
    "\n",
    "#generate the batches\n",
    "def get_batches(directory, target_size=target_size, batch_size=batch_size, shuffle=False):\n",
    "    datagen = ImageDataGenerator()\n",
    "    return datagen.flow_from_directory(directory=directory,\n",
    "                                          target_size=target_size,\n",
    "                                          batch_size=batch_size,\n",
    "                                          class_mode='categorical',\n",
    "                                          shuffle=shuffle)\n",
    "\n",
    "train_gen = ImageDataGenerator(rotation_range=0, \n",
    "                                     width_shift_range=0, \n",
    "                                     height_shift_range=0.1, \n",
    "                                     shear_range=0.1, \n",
    "                                     zoom_range=0.2, \n",
    "                                     channel_shift_range=10,\n",
    "                                     fill_mode='nearest')\n",
    "\n",
    "batches = train_gen.flow_from_directory(directory=path+'train_set',  \n",
    "                                        target_size=target_size,\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True)\n",
    "\n",
    "#batches = get_batches(path+'train_set', shuffle=True)\n",
    "\n",
    "valid_batches = get_batches(path+'valid_set', batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "nb_train_samples = batches.samples * 2\n",
    "nb_validation_samples = valid_batches.samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import AveragePooling2D\n",
    "\n",
    "# initialize the model\n",
    "initial_model = InceptionV3(weights='imagenet', include_top=False, input_shape = (3, 224, 224))\n",
    "\n",
    "x = AveragePooling2D((5,5), strides=(5, 5))(initial_model.output)\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(batches.num_class, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=initial_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we freeze the other layers \n",
    "for layer in initial_model.layers: layer.trainable=False\n",
    "print(model.summary())\n",
    "opt = Adam(lr=10e-4)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(batches, epochs=10, \n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=valid_batches, \n",
    "                    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.save('models/batchnorm_step1.h5')\n",
    "\n",
    "for layer in model.layers[:10]: \n",
    "    layer.trainable = False \n",
    "\n",
    "for layer in model.layers[10:]: \n",
    "    layer.trainable = True\n",
    "    \n",
    "opt = Adam(lr=10e-5)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, epochs=10, \n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=valid_batches, \n",
    "                    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''#model.save('models/batchnorm_ft.h5')\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size)\n",
    "preds = model.predict_generator(test_batches, steps=test_batches.samples)\n",
    "preds = preds.clip(min=0.02, max=0.98)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "filenames = test_batches.filenames\n",
    "ids = np.array([f[8:] for f in filenames])\n",
    "\n",
    "df_subm = pd.DataFrame(preds, index=ids, columns=['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9'])\n",
    "df_subm.reset_index(level=0, inplace=True)\n",
    "df_subm=df_subm.rename(columns = {'index':'img'})\n",
    "df_subm.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''from IPython.display import FileLink\n",
    "submission_file_name = 'submission.csv.gz'\n",
    "df_subm.to_csv(submission_file_name, index=False, compression='gzip')\n",
    "FileLink(submission_file_name)'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
