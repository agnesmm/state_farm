{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 40,970\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dense, Input, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "#path = '/home/ubuntu/nbs/state_farm/data/sample1/'\n",
    "#batch_size=1\n",
    "\n",
    "path = '/home/ubuntu/nbs/state_farm/data/sample/'\n",
    "#path = '/home/ubuntu/nbs/state_farm/data/'\n",
    "batch_size = 64\n",
    "\n",
    "target_size=(224, 224)\n",
    "\n",
    "nb_train_samples = 1600\n",
    "nb_validation_samples = 300\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "#generate the batches\n",
    "def get_batches(directory, target_size=target_size, batch_size=batch_size, shuffle=False):\n",
    "    datagen = ImageDataGenerator()\n",
    "    return datagen.flow_from_directory(directory=directory,\n",
    "                                          target_size=target_size,\n",
    "                                          batch_size=batch_size,\n",
    "                                          class_mode='categorical',\n",
    "                                          shuffle=shuffle)\n",
    "\n",
    "train_gen = ImageDataGenerator(rotation_range=0, \n",
    "                                     width_shift_range=0, \n",
    "                                     height_shift_range=0.1, \n",
    "                                     shear_range=0.1, \n",
    "                                     zoom_range=0.2, \n",
    "                                     channel_shift_range=10,\n",
    "                                     fill_mode='nearest')\n",
    "\n",
    "batches = train_gen.flow_from_directory(directory=path+'train_set',  \n",
    "                                        target_size=target_size,\n",
    "                                        batch_size=batch_size,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True)\n",
    "\n",
    "#batches = get_batches(path+'train_set', shuffle=True)\n",
    "\n",
    "valid_batches = get_batches(path+'valid_set', batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "# initialize the model\n",
    "initial_model = VGG16(weights=\"imagenet\", include_top=True)\n",
    "\n",
    "#finetuning\n",
    "x = Dense(batches.num_class, activation='softmax')(initial_model.layers[-2].output)\n",
    "model = Model(initial_model.input, x)\n",
    "\n",
    "# we freeze the other layers \n",
    "for layer in initial_model.layers: layer.trainable=False\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(batches, epochs=10, \n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=valid_batches, \n",
    "                    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 70s - loss: 0.3311 - acc: 0.8956 - val_loss: 2.1979 - val_acc: 0.4186\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 70s - loss: 0.2635 - acc: 0.9288 - val_loss: 2.0861 - val_acc: 0.4836\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 68s - loss: 0.2213 - acc: 0.9313 - val_loss: 2.1890 - val_acc: 0.4622\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 70s - loss: 0.1740 - acc: 0.9419 - val_loss: 2.2027 - val_acc: 0.4860\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 70s - loss: 0.1643 - acc: 0.9481 - val_loss: 2.3180 - val_acc: 0.4813\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 69s - loss: 0.1267 - acc: 0.9650 - val_loss: 2.4476 - val_acc: 0.4477\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 71s - loss: 0.1260 - acc: 0.9651 - val_loss: 2.2782 - val_acc: 0.5023\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 69s - loss: 0.1201 - acc: 0.9650 - val_loss: 2.3105 - val_acc: 0.4709\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 107s - loss: 0.1002 - acc: 0.9707 - val_loss: 2.4573 - val_acc: 0.4826\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 100s - loss: 0.1045 - acc: 0.9688 - val_loss: 2.3811 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbeec3052d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in model.layers[:10]: \n",
    "    layer.trainable = False \n",
    "\n",
    "for layer in model.layers[10:]: \n",
    "    layer.trainable = True\n",
    "    \n",
    "opt = SGD(lr=10e-5) \n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(batches, epochs=10, \n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_data=valid_batches, \n",
    "                    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# predictions on new data\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size)\n",
    "preds = model.predict_generator(test_batches, steps=test_batches.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01        0.03629445  0.01576909  0.01        0.01        0.01        0.01\n",
      "   0.94735545  0.01        0.01      ]\n",
      " [ 0.01        0.01        0.99000001  0.01        0.01        0.01        0.01\n",
      "   0.01        0.01        0.01      ]\n",
      " [ 0.76043767  0.01        0.01        0.01        0.01        0.14135244\n",
      "   0.01        0.01        0.01        0.09578142]\n",
      " [ 0.01        0.15547365  0.76144546  0.01        0.01        0.01\n",
      "   0.01636479  0.01        0.04142869  0.02074409]\n",
      " [ 0.01        0.01        0.01        0.01        0.01        0.01        0.01\n",
      "   0.9820087   0.01433661  0.01      ]\n",
      " [ 0.01        0.01        0.01        0.01        0.01        0.57158494\n",
      "   0.04882823  0.01700319  0.27842256  0.07719808]\n",
      " [ 0.99000001  0.01        0.01        0.01        0.01        0.01        0.01\n",
      "   0.01        0.01        0.01      ]\n",
      " [ 0.01639338  0.01        0.01        0.38630763  0.01        0.5032298\n",
      "   0.02658342  0.01        0.04396535  0.01841919]\n",
      " [ 0.02812427  0.01        0.01        0.01        0.95682639  0.01        0.01\n",
      "   0.01        0.01        0.01242709]\n",
      " [ 0.01        0.01        0.01        0.01        0.01        0.04037326\n",
      "   0.93818694  0.01        0.01        0.01      ]]\n"
     ]
    }
   ],
   "source": [
    "preds_clipes = preds.clip(min=0.01, max=0.99)\n",
    "print(preds_clipes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/data_augmentation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
